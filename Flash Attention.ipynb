{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "211e4892-f0d9-44b8-886d-ee5d53fbeccc",
   "metadata": {},
   "source": [
    "Our goal is to implement an efficient CUDA kernel for FlashAttention v1\n",
    "\n",
    "We will:\n",
    "1. Do a minimal FlashAttention computation by hand to understand it\n",
    "2. Implement a naive Python version\n",
    "3. Implement a Python version with CUDA-like kernels\n",
    "4. Implement a mamba version\n",
    "5. Implement a CUDA version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da775f6a-7bce-45b9-9ea8-1c575c937f7f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1: Do a minimal FlashAttention computation by hand to understand it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e63b352-2bfb-433f-b3be-df4ec7d831c0",
   "metadata": {},
   "source": [
    "This is not in obviously not in the notebook. ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0857bac0-56f9-462e-bd18-4e882cfb9a4b",
   "metadata": {},
   "source": [
    "## 2. Implement a naive Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b5441ae-8cf2-4757-a7dd-f03c7718df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import tensor, float32 as f32\n",
    "from torch.nn.functional import softmax, scaled_dot_product_attention\n",
    "\n",
    "torch.set_printoptions(sci_mode=False, precision=2, linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26555df4-a018-430d-aacf-0774bd539806",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, d = 2,1\n",
    "M = 2\n",
    "\n",
    "Q = tensor([1, 1], dtype=f32).reshape(N,d)\n",
    "K = tensor([0, 2], dtype=f32).reshape(N,d)\n",
    "V = tensor([0,-1], dtype=f32).reshape(N,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc083da8-d886-4f9c-99c8-cdd54f269a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.88],\n",
       "        [-0.88]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_dot_product_attention(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c27b93ef-0789-4168-877d-f5a5f0bc33e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.88],\n",
       "        [-0.88]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(Q@K.t(), dim=1)@V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "befa9f22-2382-4d9a-8624-e63ef9ea3069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.],\n",
       "         [0.]]),\n",
       " tensor([0., 0.]),\n",
       " tensor([-inf, -inf]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output\n",
    "O = torch.zeros(N,d)\n",
    "# softmax denominator, per row\n",
    "l = torch.zeros(N)\n",
    "# max for numerical stability, per row\n",
    "m = torch.full((N,), float('-inf'))\n",
    "\n",
    "O, l, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6bb29d4-011d-49aa-a0c2-fe7aec13c889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block sizes:   1, 1\n",
      "Block numbers: 2, 2\n"
     ]
    }
   ],
   "source": [
    "# block sizes\n",
    "bc = math.ceil(M / (4*d))\n",
    "br = min(math.ceil(M  / (4*d)), d)\n",
    "\n",
    "# block numbers\n",
    "tc = math.ceil(N / bc)\n",
    "tr = math.ceil(N / br)\n",
    "\n",
    "print(f'Block sizes:   {bc}, {br}')\n",
    "print(f'Block numbers: {tc}, {tr }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bc32670-526f-416f-b1f4-553d75ad041d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1]) torch.Size([2, 1]) torch.Size([2, 1])\n",
      "torch.Size([2, 1]) torch.Size([2, 1]) torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "# we don't want partial blocks, so pad Q, K, V with zeros\n",
    "Q_ = torch.zeros(br*tr,d)\n",
    "K_ = torch.zeros(bc*tc,d)\n",
    "V_ = torch.zeros(bc*tc,d)\n",
    "\n",
    "Q_[:N,:d] = Q\n",
    "K_[:N,:d] = K\n",
    "V_[:N,:d] = V\n",
    "\n",
    "print(Q.shape, K.shape, V.shape)\n",
    "\n",
    "Q,K,V = Q_,K_,V_\n",
    "\n",
    "print(Q.shape, K.shape, V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a81ea5d1-2982-4414-a8f0-6b821ce0380e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.88],\n",
       "        [-0.88]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(Q@K.t(), dim=1)@V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf99d0fc-3e13-428b-a225-9d60a6edda30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer loop for phase: j = 0\n",
      "\tKj = tensor([[0.]])\tVj = tensor([[0.]])\n",
      "\tInner loop for output rows: i = 0\n",
      "\t\tQi = tensor([[1.]])\tOi = tensor([[0.]])\tmi = tensor([-inf])\tli = tensor([0.])\n",
      "\t\tSij = tensor([[0.]])\tmij = tensor([0.])\tPij = tensor([[1.]])\tlij = tensor([1.])\n",
      "\t\tOi_new = tensor([[0.]])\tmi_new = tensor([0.])\tli_new = tensor([1.])\n",
      "\n",
      "\t\tResult:\n",
      "\t\tO = tensor([[0.],\n",
      "        [0.]])\tm = tensor([0., -inf])\tl = tensor([1., 0.])\n",
      "\n",
      "\tInner loop for output rows: i = 1\n",
      "\t\tQi = tensor([[1.]])\tOi = tensor([[0.]])\tmi = tensor([-inf])\tli = tensor([0.])\n",
      "\t\tSij = tensor([[0.]])\tmij = tensor([0.])\tPij = tensor([[1.]])\tlij = tensor([1.])\n",
      "\t\tOi_new = tensor([[0.]])\tmi_new = tensor([0.])\tli_new = tensor([1.])\n",
      "\n",
      "\t\tResult:\n",
      "\t\tO = tensor([[0.],\n",
      "        [0.]])\tm = tensor([0., 0.])\tl = tensor([1., 1.])\n",
      "\n",
      "Outer loop for phase: j = 1\n",
      "\tKj = tensor([[2.]])\tVj = tensor([[-1.]])\n",
      "\tInner loop for output rows: i = 0\n",
      "\t\tQi = tensor([[1.]])\tOi = tensor([[0.]])\tmi = tensor([0.])\tli = tensor([1.])\n",
      "\t\tSij = tensor([[2.]])\tmij = tensor([2.])\tPij = tensor([[1.]])\tlij = tensor([1.])\n",
      "\t\tOi_new = tensor([[-0.88]])\tmi_new = tensor([2.])\tli_new = tensor([1.14])\n",
      "\n",
      "\t\tResult:\n",
      "\t\tO = tensor([[-0.88],\n",
      "        [ 0.00]])\tm = tensor([2., 0.])\tl = tensor([1.14, 1.00])\n",
      "\n",
      "\tInner loop for output rows: i = 1\n",
      "\t\tQi = tensor([[1.]])\tOi = tensor([[0.]])\tmi = tensor([0.])\tli = tensor([1.])\n",
      "\t\tSij = tensor([[2.]])\tmij = tensor([2.])\tPij = tensor([[1.]])\tlij = tensor([1.])\n",
      "\t\tOi_new = tensor([[-0.88]])\tmi_new = tensor([2.])\tli_new = tensor([1.14])\n",
      "\n",
      "\t\tResult:\n",
      "\t\tO = tensor([[-0.88],\n",
      "        [-0.88]])\tm = tensor([2., 2.])\tl = tensor([1.14, 1.14])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop for phase\n",
    "for j in range(0, tc):\n",
    "    print(f'Outer loop for phase: {j = }')\n",
    "    \n",
    "    # load from HBM -> SRAM\n",
    "    Kj = K[j*bc:(j+1)*bc]\n",
    "    Vj = V[j*bc:(j+1)*bc]    \n",
    "\n",
    "    assert Kj.shape==(bc,d)\n",
    "    assert Vj.shape==(bc,d)\n",
    "\n",
    "    print(f'\\t{Kj = }\\t{Vj = }')\n",
    "\n",
    "    # Loop for output rows\n",
    "    for i in range(0, tr):\n",
    "        print(f'\\tInner loop for output rows: {i = }')\n",
    "    \n",
    "        # load from HBM -> SRAM\n",
    "        Qi = Q[i*br:(i+1)*br]\n",
    "        Oi = O[i*br:(i+1)*br]\n",
    "        mi = m[i*br:(i+1)*br]\n",
    "        li = l[i*br:(i+1)*br]\n",
    "\n",
    "        print(f'\\t\\t{Qi = }\\t{Oi = }\\t{mi = }\\t{li = }')\n",
    "        \n",
    "        assert Qi.shape==(br,d)\n",
    "        assert Oi.shape==(br,d)\n",
    "        assert li.shape==(br,)\n",
    "        assert mi.shape==(br,)\n",
    "\n",
    "        # compute\n",
    "        Sij = Qi@Kj.t() / math.sqrt(d)\n",
    "        assert Sij.shape==(br,bc)\n",
    "\n",
    "        mij = Sij.max(1).values\n",
    "        assert mij.shape==(br,)\n",
    "        \n",
    "        Pij = (Sij - mij[:,None]).exp()\n",
    "        assert Pij.shape==(br,bc)\n",
    "\n",
    "        lij = Pij.sum(1)\n",
    "        assert lij.shape==(br,)\n",
    "\n",
    "        print(f'\\t\\t{Sij = }\\t{mij = }\\t{Pij = }\\t{lij = }')\n",
    "        \n",
    "        mi_new = max(mi, mij)\n",
    "        li_new = (mi-mi_new).exp()*li + (mij-mi_new).exp()*lij\n",
    "        assert mi_new.shape==(br,)\n",
    "        assert li_new.shape==(br,)\n",
    "\n",
    "        Oi_new_part1 = torch.diag(li)@(mi - mi_new).exp() # br,br   @ br   -> br\n",
    "        Oi_new_part1 = Oi_new_part1[:,None] * Oi          # br,None * br,d -> br,d\n",
    "        assert Oi_new_part1.shape==(br,d)        \n",
    "        Oi_new_part2 = (mij-mi_new).exp()[:,None]*Pij     # br,None * br,bc -> br,bc\n",
    "        Oi_new_part2 = Oi_new_part2@Vj                    # br,bc   @ bc,d  -> br,d\n",
    "        assert Oi_new_part2.shape==(br,d)\n",
    "        Oi_new = Oi_new_part1 + Oi_new_part2              # br,d + br,d  -> br,d\n",
    "        Oi_new = torch.diag(li_new).inverse() @ Oi_new    # br,br @ br,d -> br,d        \n",
    "        assert Oi_new.shape==(br,d)\n",
    "\n",
    "        print(f'\\t\\t{Oi_new = }\\t{mi_new = }\\t{li_new = }')\n",
    "        \n",
    "        # Write SRAM -> HBM\n",
    "        O[i*br:(i+1)*br] = Oi_new\n",
    "        m[i*br:(i+1)*br] = mi_new\n",
    "        l[i*br:(i+1)*br] = li_new\n",
    "\n",
    "        print('\\n\\t\\tResult:')\n",
    "        print(f'\\t\\t{O = }\\t{m = }\\t{l = }')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05981d3e-b145-4eae-885b-c263d7e0c48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.88],\n",
       "        [-0.88]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d69a92d1-926e-4b63-b090-f498a34bd014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(O==scaled_dot_product_attention(Q,K,V)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f32d9e3-e365-4b4e-b81a-397ce99be24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del Q,K,V,O,N,d,M,m,l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "874e9995-07bb-4da1-8276-47b3aa71afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def flash_attention(Q,K,V,M=10,verbose=False):\n",
    "    # Q,K,V = attn matrices; M = shared mem size\n",
    "    N,d = Q.shape\n",
    "    assert V.shape==K.shape==(N,d), \"Shape mismatch\"\n",
    "    \n",
    "    # block sizes\n",
    "    bc = math.ceil(M / (4*d))\n",
    "    br = min(math.ceil(M / (4*d)), d)\n",
    "    # block numbers\n",
    "    tc = math.ceil(N / bc)\n",
    "    tr = math.ceil(N / br)    \n",
    "\n",
    "    print(f'Block sizes:   {bc}, {br}')\n",
    "    print(f'Block numbers: {tc}, {tr }')\n",
    "    \n",
    "    O = torch.zeros(N,d)                 # output\n",
    "    l = torch.zeros(N)                   # softmax denominator, per row\n",
    "    m = torch.full((N,), float('-inf'))  # max for numerical stability, per row\n",
    "\n",
    "    with tqdm(total=tc*tr) as pbar:\n",
    "        # Loop for phase\n",
    "        for j in range(0, tc):\n",
    "            if verbose: print(f'Outer loop for phase: {j = }')\n",
    "            # load from HBM -> SRAM\n",
    "            Kj = K[j*bc:(j+1)*bc]\n",
    "            Vj = V[j*bc:(j+1)*bc]    \n",
    "\n",
    "            if verbose: print(f'\\t{Kj = }\\n\\t{Vj = }')    \n",
    "\n",
    "            # Loop for output rows\n",
    "            for i in range(0, tr):\n",
    "                if verbose: print(f'\\tInner loop for output rows: {i = }') \n",
    "                # load from HBM -> SRAM\n",
    "                Qi = Q[i*br:(i+1)*br]\n",
    "                Oi = O[i*br:(i+1)*br]\n",
    "                mi = m[i*br:(i+1)*br]\n",
    "                li = l[i*br:(i+1)*br]\n",
    "\n",
    "                if verbose: print(f'\\t\\t{Qi = }\\n\\t\\t{Oi = }\\n\\t\\t{mi = }\\n\\t\\t{li = }')\n",
    "                        \n",
    "                # compute\n",
    "                Sij = Qi@Kj.t() / math.sqrt(d)\n",
    "                mij = Sij.max(1).values\n",
    "                Pij = (Sij - mij[:,None]).exp()\n",
    "                lij = Pij.sum(1)\n",
    "\n",
    "                if verbose: print(f'\\t\\t{Sij = }\\n\\t\\t{mij = }\\n\\t\\t{Pij = }\\n\\t\\t{lij = }')\n",
    "                \n",
    "                mi_new = torch.max(mi, mij)\n",
    "                li_new = (mi-mi_new).exp()*li + (mij-mi_new).exp()*lij\n",
    "\n",
    "                Oi_new_part1 = torch.diag(li)@(mi - mi_new).exp() # br,br   @ br   -> br\n",
    "                Oi_new_part1 = Oi_new_part1[:,None] * Oi          # br,None * br,d -> br,d\n",
    "                assert Oi_new_part1.shape==(br,d)        \n",
    "                Oi_new_part2 = (mij-mi_new).exp()[:,None]*Pij     # br,None * br,bc -> br,bc\n",
    "                Oi_new_part2 = Oi_new_part2@Vj                    # br,bc   @ bc,d  -> br,d\n",
    "                assert Oi_new_part2.shape==(br,d)\n",
    "                Oi_new = Oi_new_part1 + Oi_new_part2              # br,d + br,d  -> br,d\n",
    "                Oi_new = torch.diag(li_new).inverse() @ Oi_new    # br,br @ br,d -> br,d        \n",
    "                assert Oi_new.shape==(br,d)\n",
    "\n",
    "                if verbose: print(f'\\t\\t{Oi_new = }\\n\\t\\t{mi_new = }\\n\\t\\t{li_new = }')\n",
    "                            \n",
    "                # Write SRAM -> HBM\n",
    "                O[i*br:(i+1)*br] = Oi_new\n",
    "                m[i*br:(i+1)*br] = mi_new\n",
    "                l[i*br:(i+1)*br] = li_new\n",
    "\n",
    "                if verbose: print(f'\\n\\t\\tResult:\\n\\t\\t{O = }\\t{m = }\\t{l = }\\n')\n",
    "                \n",
    "                pbar.update(1)\n",
    "\n",
    "    return O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bb66d14-f638-4404-a375-8efe576a9ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block sizes:   3, 1\n",
      "Block numbers: 1, 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c007bbf1964cc8a1fb90d9baa0e25b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Q = tensor([1, 1], dtype=f32).reshape(2,1)\n",
    "K = tensor([0, 2], dtype=f32).reshape(2,1)\n",
    "V = tensor([0,-1], dtype=f32).reshape(2,1)\n",
    "\n",
    "O = flash_attention(Q,K,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e83955ca-bda4-410f-b7a2-5a88b7f5ab3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(O, scaled_dot_product_attention(Q,K,V)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f1cc96-4178-4871-90f5-99c046df3ef7",
   "metadata": {},
   "source": [
    "A minimal example that didn't work (and now does):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b73a5a42-7bcf-43b3-b307-b446c3ec905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block sizes:   2, 2\n",
      "Block numbers: 1, 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21d7bae3c894850ac49259f99f09e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = tensor([1,0,0,0], dtype=f32).view(2,2)\n",
    "K = tensor([1,0,0,0], dtype=f32).view(2,2)\n",
    "V = tensor([0,1,0,0], dtype=f32).view(2,2)\n",
    "\n",
    "O = flash_attention(Q,K,V,M=9)\n",
    "\n",
    "torch.isclose(O, scaled_dot_product_attention(Q,K,V)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29fc4e7-c638-42c5-948c-ec82ea7f2a40",
   "metadata": {},
   "source": [
    "A random 2x2 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa4a56f7-25d2-457d-99b5-17f3055bb909",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.rand(2,2, dtype=f32)\n",
    "K = torch.rand(2,2, dtype=f32)\n",
    "V = torch.rand(2,2, dtype=f32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f19fec73-c6ce-427f-99e8-40b08fedae3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block sizes:   1, 1\n",
      "Block numbers: 2, 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603da858b3b8430ebf9212a0f16e00fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "O = flash_attention(Q,K,V,M=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1aa02321-0b6d-49ec-afcd-33d61c14e1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(O, scaled_dot_product_attention(Q,K,V)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee0e84c-65bc-481b-94a8-0b0f3cb8ae97",
   "metadata": {},
   "source": [
    "A random, larger matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fc00d5e-19a8-49f5-aa71-b45b4432ef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.rand(256,64, dtype=f32)\n",
    "K = torch.rand(256,64, dtype=f32)\n",
    "V = torch.rand(256,64, dtype=f32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9de4b776-0b63-4b7a-8cc8-7542c546f19d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block sizes:   1, 1\n",
      "Block numbers: 256, 256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3584ce00844f7597df198a538b04b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65536 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "O = flash_attention(Q,K,V,M=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4118da2-19d0-4cca-a39b-70770f029b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(O, scaled_dot_product_attention(Q,K,V)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b2a7a2-25e8-4d7d-92d3-0ddf4bde4577",
   "metadata": {},
   "source": [
    "## 3. Implement a Python version with CUDA-like kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1220de8b-1218-4ae5-a21a-0a5aa506b274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import tensor, float32 as f32\n",
    "from torch.nn.functional import softmax, scaled_dot_product_attention\n",
    "\n",
    "torch.set_printoptions(sci_mode=False, precision=2, linewidth=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec05041-f8ea-4e92-b9a8-20e5c8afe7aa",
   "metadata": {},
   "source": [
    "This kernel runner is taken from \n",
    "> https://github.com/cuda-mode/lectures/blob/main/lecture5/matmul_l5.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dee6d465-4280-4326-8663-798d9e4889fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "dim3 = namedtuple('dim3', ['x','y','z'], defaults=(1,1))\n",
    "def cdiv(a,b): return (a + b - 1) // b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63ed75e4-291b-4c42-88d0-5f720722619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread, Barrier\n",
    "\n",
    "def blk_kernel2d_shar(f, blocks, tpb, sh_sz, *args, **kwargs):\n",
    "    for i0 in range(blocks.y):\n",
    "        for i1 in range(blocks.x):\n",
    "            shar = torch.zeros(sh_sz)\n",
    "            syncb = Barrier(tpb.y*tpb.x)\n",
    "            threads = [\n",
    "                Thread(\n",
    "                    target=f,\n",
    "                    args=(dim3(i1,i0), dim3(p,o), tpb, shar, syncb, *args),\n",
    "                    kwargs=kwargs\n",
    "                )\n",
    "                for o in range(tpb.y)\n",
    "                for p in range(tpb.x)\n",
    "            ]\n",
    "            for tr in threads: tr.start()\n",
    "            for tr in threads: tr.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3113926-4bcf-490f-95bf-a0dfeecc4214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attn(kernel_func, Q, K, V, sh_sz):\n",
    "    # Q,K,V = attn matrices; sh_sz = shared mem size; n x d = shape of Q/K/V/O\n",
    "    n,d = Q.shape\n",
    "    assert V.shape==K.shape==(n,d), \"Shape mismatch\"\n",
    "\n",
    "    O = torch.zeros(n,d)                 # output\n",
    "    l = torch.zeros(n)                   # softmax denominator, per row\n",
    "    m = torch.full((n,), float('-inf'))  # max for numerical stability, per row\n",
    "    \n",
    "    # block sizes\n",
    "    bc = cdiv(sh_sz, 4*d)\n",
    "    br = min(cdiv(sh_sz, 4*d), d)\n",
    "    block_sizes = dim3(bc,br)\n",
    "    \n",
    "    # block numbers\n",
    "    tc = cdiv(n, bc)\n",
    "    tr = cdiv(n, br)\n",
    "    block_numbers = dim3(tc,tr)\n",
    "    \n",
    "    print(f'Block sizes:   {bc}, {br}')\n",
    "    print(f'Block numbers: {tc}, {tr }')\n",
    "        \n",
    "    blk_kernel2d_shar(\n",
    "        f=kernel_func,\n",
    "        blocks=block_numbers, tpb=block_sizes,\n",
    "        sh_sz=sh_sz,\n",
    "        Q=Q.flatten(), K=K.flatten(), V=V.flatten(), O=O.flatten(),\n",
    "        m=m,l=l,\n",
    "        n=n, d=d\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d81d074-b610-48f1-a0bd-72771cef12ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = tensor([1,0,0,0], dtype=f32).view(2,2)\n",
    "K = tensor([1,0,0,0], dtype=f32).view(2,2)\n",
    "V = tensor([0,1,0,0], dtype=f32).view(2,2)\n",
    "sh_sz = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f83ff82-78e5-4f9e-9e15-cebb7c3f6a03",
   "metadata": {},
   "source": [
    "We need to decide how to split up shared memory. Let's just store the objects in SRAM by order of read (`Kj`, `Vj`, `Qi`, `Oi`, `mi`, `li`).\n",
    "\n",
    "Then the offsets for each are:\n",
    "- `Kj`: `0`\n",
    "- `Vj`: `bc * d`\n",
    "- `Qi`: `2*bc*d`\n",
    "- `Oi`: `2*bc*d + br*d`\n",
    "- `mi`: `2*bc*d + 2*br*d`\n",
    "- `li`: `2*bc*d + 2*br*d + br`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "285559a1-1ea8-4b60-91f1-bb000427828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attn_kernel(blockIdx, threadIdx, blockDim, shared, syncb, Q, K, V, O, m, l, n, d):\n",
    "    # Q,K,V = attn matrices; O = output; m,l = running statistic; n x d = shape of Q/K/V/O\n",
    "\n",
    "    print(f'Executing kernel: {blockIdx = } , {threadIdx = }')\n",
    "\n",
    "    br,bc,_ = blockDim\n",
    "    \n",
    "    # split SRAM\n",
    "    Kj = shared[                     : bc+d                ]\n",
    "    Vj = shared[  bc+d               : 2*bc*d              ]\n",
    "    Qi = shared[2*bc*d               : 2*bc*d +   br*d     ]\n",
    "    Oi = shared[2*bc*d +   br*d      : 2*bc*d + 2*br*d     ]\n",
    "    mi = shared[2*bc*d + 2*br*d      : 2*bc*d + 2*br*d + br]\n",
    "    li = shared[2*bc*d + 2*br*d + br :                     ]\n",
    "        \n",
    "    for j in range(0, tc):\n",
    "        # load from HBM -> SRAM\n",
    "        Kj = K[j*bc:(j+1)*bc]\n",
    "        Vj = V[j*bc:(j+1)*bc]\n",
    "\n",
    "        for i in range(0, tr):\n",
    "            # load from HBM -> SRAM\n",
    "            Qi = Q[i*br : (i+1)*br]  \n",
    "            Oi = O[i*br : (i+1)*br]    \n",
    "            mi = m[i*br:(i+1)*br]\n",
    "            li = l[i*br:(i+1)*br]\n",
    "\n",
    "            # currently, loading is done by each thread. how can we split loading?\n",
    "            \n",
    "            for n,o in zip('Kj, Vj, Qi, Oi, mi, li'.split(', '), [Kj, Vj, Qi, Oi, mi, li]): print(n, o)\n",
    "            return\n",
    "            \n",
    "            # compute\n",
    "            Sij = Qi@Kj.t() / math.sqrt(d)\n",
    "            mij = Sij.max(1).values\n",
    "            Pij = (Sij - mij[:,None]).exp()\n",
    "            lij = Pij.sum(1)\n",
    "\n",
    "            mi_new = torch.max(mi, mij)\n",
    "            li_new = (mi-mi_new).exp()*li + (mij-mi_new).exp()*lij\n",
    "\n",
    "            Oi_new_part1 = torch.diag(li)@(mi - mi_new).exp() # br,br   @ br   -> br\n",
    "            Oi_new_part1 = Oi_new_part1[:,None] * Oi          # br,None * br,d -> br,d\n",
    "            assert Oi_new_part1.shape==(br,d)        \n",
    "            Oi_new_part2 = (mij-mi_new).exp()[:,None]*Pij     # br,None * br,bc -> br,bc\n",
    "            Oi_new_part2 = Oi_new_part2@Vj                    # br,bc   @ bc,d  -> br,d\n",
    "            assert Oi_new_part2.shape==(br,d)\n",
    "            Oi_new = Oi_new_part1 + Oi_new_part2              # br,d + br,d  -> br,d\n",
    "            Oi_new = torch.diag(li_new).inverse() @ Oi_new    # br,br @ br,d -> br,d        \n",
    "            assert Oi_new.shape==(br,d)\n",
    "\n",
    "            # Write SRAM -> HBM\n",
    "            O[i*br:(i+1)*br] = Oi_new\n",
    "            m[i*br:(i+1)*br] = mi_new\n",
    "            l[i*br:(i+1)*br] = li_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2cecfc2-f1f1-4222-8cda-45e79615c4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block sizes:   2, 2\n",
      "Block numbers: 1, 1\n",
      "Executing kernel: blockIdx = dim3(x=0, y=0, z=1) , threadIdx = dim3(x=0, y=0, z=1)\n",
      "Kj Executing kernel: blockIdx = dim3(x=0, y=0, z=1) , threadIdx = dim3(x=1, y=0, z=1)\n",
      "Kj Executing kernel: blockIdx = dim3(x=0, y=0, z=1) , threadIdx = dim3(x=0, y=1, z=1)\n",
      "Kj Executing kernel: blockIdx = dim3(x=0, y=0, z=1) , threadIdx = dim3(x=1, y=1, z=1)\n",
      "Kj tensor([1., 0.])\n",
      "Vj tensor([1., 0.])\n",
      "Vj tensor([1., 0.])\n",
      "Vj tensor([1., 0.])\n",
      "Vj tensor([0., 1.])\n",
      "Qi tensor([0., 1.])\n",
      "Qi tensor([0., 1.])\n",
      "Qi tensor([0., 1.])\n",
      "Qi tensor([1., 0.])\n",
      "Oi tensor([1., 0.])\n",
      "Oi tensor([1., 0.])\n",
      "Oi tensor([0., 0.])\n",
      "mi tensor([0., 0.])\n",
      "mi tensor([0., 0.])\n",
      "mi tensor([-inf, -inf])\n",
      "li tensor([-inf, -inf])\n",
      "li tensor([-inf, -inf])\n",
      "li tensor([1., 0.])\n",
      "Oi tensor([0., 0.])\n",
      "tensor([0., 0.])\n",
      "tensor([0., 0.])\n",
      "mi tensor([-inf, -inf])\n",
      "li tensor([0., 0.])\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "flash_attn(flash_attn_kernel, Q, K, V, sh_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05604702-67fd-45ce-b2f5-0e1adb803153",
   "metadata": {},
   "source": [
    "The inputs Q,K,V,O are flattened, so we need to changed access to them from 2d into 1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24ac044a-e5af-4291-8215-1796f07b5876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attn_kernel(blockIdx, threadIdx, blockDim, shared, syncb, Q, K, V, O, m, l, n, d):\n",
    "    # Q,K,V = attn matrices; O = output; m,l = running statistic; n x d = shape of Q/K/V/O\n",
    "\n",
    "    print(f'Executing kernel: {blockIdx = } , {threadIdx = }')\n",
    "\n",
    "    br,bc,_ = blockDim\n",
    "    \n",
    "    # split SRAM\n",
    "    Kj = shared[                     : bc+d                ]\n",
    "    Vj = shared[  bc+d               : 2*bc*d              ]\n",
    "    Qi = shared[2*bc*d               : 2*bc*d +   br*d     ]\n",
    "    Oi = shared[2*bc*d +   br*d      : 2*bc*d + 2*br*d     ]\n",
    "    mi = shared[2*bc*d + 2*br*d      : 2*bc*d + 2*br*d + br]\n",
    "    li = shared[2*bc*d + 2*br*d + br :                     ]\n",
    "        \n",
    "    for j in range(0, tc):\n",
    "        # load from HBM -> SRAM\n",
    "        # remember, everything is 1d\n",
    "        Kj = K[j*n*bc:(j+1)*n*bc]\n",
    "        Vj = V[j*n*bc:(j+1)*n*bc]\n",
    "\n",
    "        for i in range(0, tr):\n",
    "            # load from HBM -> SRAM\n",
    "            # remember, everything is 1d\n",
    "            Qi = Q[i*n*br : (i+1)*n*br]  \n",
    "            Oi = O[i*n*br : (i+1)*n*br]    \n",
    "            mi = m[i*br:(i+1)*br]\n",
    "            li = l[i*br:(i+1)*br]\n",
    "\n",
    "            # currently, loading is done by each thread. how can we split loading?\n",
    "            \n",
    "            for n,o in zip('Kj, Vj, Qi, Oi, mi, li'.split(', '), [Kj, Vj, Qi, Oi, mi, li]): print(n, o)\n",
    "            return\n",
    "            \n",
    "            # compute\n",
    "            Sij = Qi@Kj.t() / math.sqrt(d)\n",
    "            mij = Sij.max(1).values\n",
    "            Pij = (Sij - mij[:,None]).exp()\n",
    "            lij = Pij.sum(1)\n",
    "\n",
    "            mi_new = torch.max(mi, mij)\n",
    "            li_new = (mi-mi_new).exp()*li + (mij-mi_new).exp()*lij\n",
    "\n",
    "            Oi_new_part1 = torch.diag(li)@(mi - mi_new).exp() # br,br   @ br   -> br\n",
    "            Oi_new_part1 = Oi_new_part1[:,None] * Oi          # br,None * br,d -> br,d\n",
    "            assert Oi_new_part1.shape==(br,d)        \n",
    "            Oi_new_part2 = (mij-mi_new).exp()[:,None]*Pij     # br,None * br,bc -> br,bc\n",
    "            Oi_new_part2 = Oi_new_part2@Vj                    # br,bc   @ bc,d  -> br,d\n",
    "            assert Oi_new_part2.shape==(br,d)\n",
    "            Oi_new = Oi_new_part1 + Oi_new_part2              # br,d + br,d  -> br,d\n",
    "            Oi_new = torch.diag(li_new).inverse() @ Oi_new    # br,br @ br,d -> br,d        \n",
    "            assert Oi_new.shape==(br,d)\n",
    "\n",
    "            # Write SRAM -> HBM\n",
    "            O[i*br:(i+1)*br] = Oi_new\n",
    "            m[i*br:(i+1)*br] = mi_new\n",
    "            l[i*br:(i+1)*br] = li_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2863d56b-ada3-42e9-9951-a586f6cdf102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block sizes:   2, 2\n",
      "Block numbers: 1, 1\n",
      "Executing kernel: blockIdx = dim3(x=0, y=0, z=1) , threadIdx = dim3(x=0, y=0, z=1)\n",
      "Kj Executing kernel: blockIdx = dim3(x=0, y=0, z=1) , threadIdx = dim3(x=1, y=0, z=1)\n",
      "Kj Executing kernel: blockIdx = dim3(x=0, y=0, z=1) , threadIdx = dim3(x=0, y=1, z=1)\n",
      "Kj Executing kernel: blockIdx = dim3(x=0, y=0, z=1) , threadIdx = dim3(x=1, y=1, z=1)\n",
      "Kj tensor([1., 0., 0., 0.])\n",
      "Vj tensor([1., 0., 0., 0.])\n",
      "Vj tensor([1., 0., 0., 0.])\n",
      "Vj tensor([1., 0., 0., 0.])\n",
      "Vj tensor([0., 1., 0., 0.])\n",
      "Qi tensor([0., 1., 0., 0.])\n",
      "Qi tensor([0., 1., 0., 0.])\n",
      "Qi tensor([0., 1., 0., 0.])\n",
      "Qi tensor([1., 0., 0., 0.])\n",
      "Oi tensor([0., 0., 0., 0.])\n",
      "mi tensor([1., 0., 0., 0.])\n",
      "Oi tensor([1., 0., 0., 0.])\n",
      "Oi tensor([1., 0., 0., 0.])\n",
      "Oi tensor([0., 0., 0., 0.])\n",
      "mi tensor([0., 0., 0., 0.])\n",
      "mi tensor([-inf, -inf])\n",
      "li tensor([0., 0., 0., 0.])\n",
      "mi tensor([-inf, -inf])\n",
      "li tensor([-inf, -inf])\n",
      "li tensor([0., 0.])\n",
      "tensor([0., 0.])\n",
      "tensor([0., 0.])\n",
      "tensor([-inf, -inf])\n",
      "li tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "flash_attn(flash_attn_kernel, Q, K, V, sh_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2181a758-c780-4e1e-995b-509671d53592",
   "metadata": {},
   "source": [
    "To Do:\n",
    "- Split loading into shared memeory accros threads\n",
    "- Figure out how to split comutation accross threads\n",
    "- Add guards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825b3118-4fbc-40ee-9684-d9dd78a61042",
   "metadata": {},
   "source": [
    "Let's first split the computation naively: Each computation step will be split separately, and we sync in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f01445c-dec0-409d-9d02-af7034a9a5aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ')' does not match opening parenthesis '[' (1398450284.py, line 46)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[32], line 46\u001b[0;36m\u001b[0m\n\u001b[0;31m    for ctn_d in range(d): mij[ty] = max(mij[ty], Sij[ty][ctn_d)]\u001b[0m\n\u001b[0m                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m closing parenthesis ')' does not match opening parenthesis '['\n"
     ]
    }
   ],
   "source": [
    "def flash_attn_kernel(blockIdx, threadIdx, blockDim, shared, syncb, Q, K, V, O, m, l, n, d):\n",
    "    # Q,K,V = attn matrices; O = output; m,l = running statistic; n x d = shape of Q/K/V/O\n",
    "\n",
    "    print(f'Executing kernel: {blockIdx = } , {threadIdx = }')\n",
    "\n",
    "    # do idx start at highest (ie z) or lowest (ie x) dim?\n",
    "    bx,by,_ = blockIdx\n",
    "    tx,ty,_ = threadIdx\n",
    "    \n",
    "    br,bc,_ = blockDim\n",
    "    \n",
    "    # split SRAM\n",
    "    Kj = shared[                     : bc+d                ]\n",
    "    Vj = shared[  bc+d               : 2*bc*d              ]\n",
    "    Qi = shared[2*bc*d               : 2*bc*d +   br*d     ]\n",
    "    Oi = shared[2*bc*d +   br*d      : 2*bc*d + 2*br*d     ]\n",
    "    mi = shared[2*bc*d + 2*br*d      : 2*bc*d + 2*br*d + br]\n",
    "    li = shared[2*bc*d + 2*br*d + br :                     ]\n",
    "        \n",
    "    for j in range(0, tc):\n",
    "        # load from HBM -> SRAM\n",
    "        # remember, everything is 1d\n",
    "        Kj = K[j*n*bc:(j+1)*n*bc]\n",
    "        Vj = V[j*n*bc:(j+1)*n*bc]\n",
    "\n",
    "        for i in range(0, tr):\n",
    "            # load from HBM -> SRAM\n",
    "            # remember, everything is 1d\n",
    "            Qi = Q[i*n*br : (i+1)*n*br]  \n",
    "            Oi = O[i*n*br : (i+1)*n*br]    \n",
    "            mi = m[i*br:(i+1)*br]\n",
    "            li = l[i*br:(i+1)*br]\n",
    "            # currently, loading is done by each thread. how can we split loading?\n",
    "\n",
    "            \n",
    "            # compute\n",
    "            # Q: Where are the intermediate variables (Sij, mij, ...) allocated? In SRAM?\n",
    "\n",
    "            # Sij = Qi@Kj.t() / math.sqrt(d)\n",
    "            for ctn_d in range(d): Sij[tx,ty] += Qi[ty][ctn_d]*Kj[tx][ctn_d]\n",
    "            Sij[tx,ty] /= math.sqrt(d)\n",
    "            syncb.wait()     \n",
    "\n",
    "            # mij = Sij.max(1).values\n",
    "            mij[ty] = 0\n",
    "            for ctn_d in range(d): mij[ty] = max(mij[ty], Sij[ty][ctn_d)]\n",
    "            syncb.wait()\n",
    "\n",
    "            continue\n",
    "            \n",
    "            Pij = (Sij - mij[:,None]).exp()\n",
    "            syncb.wait()\n",
    "            \n",
    "            lij = Pij.sum(1)\n",
    "            syncb.wait()\n",
    "            \n",
    "            mi_new = torch.max(mi, mij)\n",
    "            li_new = (mi-mi_new).exp()*li + (mij-mi_new).exp()*lij\n",
    "\n",
    "            Oi_new_part1 = torch.diag(li)@(mi - mi_new).exp() # br,br   @ br   -> br\n",
    "            Oi_new_part1 = Oi_new_part1[:,None] * Oi          # br,None * br,d -> br,d\n",
    "            assert Oi_new_part1.shape==(br,d)        \n",
    "            Oi_new_part2 = (mij-mi_new).exp()[:,None]*Pij     # br,None * br,bc -> br,bc\n",
    "            Oi_new_part2 = Oi_new_part2@Vj                    # br,bc   @ bc,d  -> br,d\n",
    "            assert Oi_new_part2.shape==(br,d)\n",
    "            Oi_new = Oi_new_part1 + Oi_new_part2              # br,d + br,d  -> br,d\n",
    "            Oi_new = torch.diag(li_new).inverse() @ Oi_new    # br,br @ br,d -> br,d        \n",
    "            assert Oi_new.shape==(br,d)\n",
    "\n",
    "            # Write SRAM -> HBM\n",
    "            O[i*br:(i+1)*br] = Oi_new\n",
    "            m[i*br:(i+1)*br] = mi_new\n",
    "            l[i*br:(i+1)*br] = li_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a4a5e-c9fc-480c-af06-8dd0fadbe045",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand(3,2)\n",
    "B = torch.rand(3,2)\n",
    "\n",
    "C = torch.zeros(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f79f4-46dc-41ad-9512-d3dc112bfa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "n = A.shape[0]\n",
    "for a,b in product(range(n), range(n)):\n",
    "    for c in range(d):\n",
    "        C[a,b] += A[a,c]*B[b,c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ac8009-410b-41a4-894e-818509e224c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.isclose(C, A@B.t()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b39579-97ca-4efd-a58c-0b18750d5ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa9119a-8733-46db-b2be-4bc23421e60b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fa19c9-bf45-4f22-8faf-6af2b3598ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
